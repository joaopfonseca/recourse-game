{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efbc70b-72dc-4f23-96f9-648d2e5a4b85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from mlresearch.utils import set_matplotlib_style, parallel_loop\n",
    "from mlresearch.utils._check_pipelines import check_random_states\n",
    "from recgame.environments import BaseEnvironment\n",
    "\n",
    "set_matplotlib_style(32, **{\"font.family\":\"Times\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2202c479-c5e2-4acb-9427-fd51b23e5e90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# NFeatureRecourse ignoring categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b51a70-d19f-493e-8696-2f1edf0b575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "from recgame.recourse.base import BaseRecourse\n",
    "\n",
    "\n",
    "class NFeatureRecourse(BaseRecourse):\n",
    "    \"\"\"TODO: Add documentation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        n_features: int = None,\n",
    "        threshold=0.5,\n",
    "        categorical: Union[list, np.ndarray] = None,\n",
    "        immutable: Union[list, np.ndarray] = None,\n",
    "        step_direction: dict = None,\n",
    "        y_desired: Union[int, str] = 1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            threshold=threshold,\n",
    "            categorical=categorical,\n",
    "            immutable=immutable,\n",
    "            step_direction=step_direction,\n",
    "            y_desired=y_desired,\n",
    "        )\n",
    "\n",
    "        # if categorical is not None and categorical != []:\n",
    "        #     raise TypeError(\n",
    "        #         \"NFeatureRecourse does not work with categorical features. Consider \"\n",
    "        #         \"using a different recourse method.\"\n",
    "        #     )\n",
    "\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def _counterfactual(self, agent, action_set):\n",
    "        agent_original = agent.copy()\n",
    "\n",
    "        # Do not change if the agent is over the threshold\n",
    "        if self.model.predict_proba(agent.to_frame().T)[0, -1] >= self.threshold:\n",
    "            return agent_original\n",
    "\n",
    "        categorical_vals = agent_original[self.categorical].values\n",
    "        agent = agent_original.drop(self.categorical).copy()\n",
    "\n",
    "        intercept, coefficients, model = self._get_coefficients()\n",
    "\n",
    "        # Get base vector\n",
    "        base_vector = coefficients.copy().squeeze()\n",
    "        n_features = (\n",
    "            base_vector.shape[0] if self.n_features is None else self.n_features\n",
    "        )\n",
    "\n",
    "        is_usable = np.array(\n",
    "            [\n",
    "                action_set[col].step_direction in [np.sign(coeff), 0]\n",
    "                and action_set[col].actionable\n",
    "                for col, coeff in zip(agent.index, base_vector)\n",
    "            ]\n",
    "        )\n",
    "        base_vector[~is_usable] = 0\n",
    "\n",
    "        # Use features with highest contribution towards the threshold\n",
    "        rejected_features = np.argsort(np.abs(base_vector))[:-n_features]\n",
    "        base_vector[rejected_features] = 0\n",
    "\n",
    "        base_vector = base_vector / np.linalg.norm(base_vector)\n",
    "        multiplier = (-intercept - np.dot(agent.values, coefficients.T)) / np.dot(\n",
    "            base_vector, coefficients.T\n",
    "        )\n",
    "        counterfactual = agent + multiplier * base_vector\n",
    "                        \n",
    "        lb, ub = np.array(action_set.lb), np.array(action_set.ub)\n",
    "        \n",
    "        lb = lb[action_set.df.name.values != self.categorical]\n",
    "        ub = ub[action_set.df.name.values != self.categorical]\n",
    "\n",
    "        # Check if base_vector adjustments are not generating invalid counterfactuals\n",
    "        for i in range(agent.shape[0]):\n",
    "            # Adjust vector according to features' bounds\n",
    "            lb_valid = counterfactual >= lb\n",
    "            ub_valid = counterfactual <= ub\n",
    "\n",
    "            if lb_valid.all() and ub_valid.all():\n",
    "                break\n",
    "\n",
    "            if not lb_valid.all():\n",
    "                # Fix values to its lower bound\n",
    "                idx = np.where(~lb_valid)[0]\n",
    "                agent[idx] = lb[idx]\n",
    "                base_vector[idx] = 0\n",
    "\n",
    "            if not ub_valid.all():\n",
    "                # Fix values to its upper bound\n",
    "                idx = np.where(~ub_valid)[0]\n",
    "                agent[idx] = ub[idx]\n",
    "                base_vector[idx] = 0\n",
    "\n",
    "            if (base_vector == 0).all():\n",
    "                # All max/min boundaries have been met.\n",
    "                counterfactual = agent\n",
    "            else:\n",
    "                \n",
    "                # Redefine counterfactual after adjusting the base vector\n",
    "                base_vector = base_vector / np.linalg.norm(base_vector)\n",
    "                multiplier = (\n",
    "                    -intercept - np.dot(agent.values, coefficients.T)\n",
    "                ) / np.dot(base_vector, coefficients.T)\n",
    "                counterfactual = agent + multiplier * base_vector\n",
    "\n",
    "        lb_valid = counterfactual >= lb\n",
    "        ub_valid = counterfactual <= ub\n",
    "        if not (lb_valid.all() and ub_valid.all()):\n",
    "            warnings.warn(\n",
    "                \"Could not generate a counterfactual to reach the desired threshold.\"\n",
    "            )\n",
    "\n",
    "            \n",
    "        for cat_feat, value in zip(self.categorical, categorical_vals):\n",
    "            counterfactual[cat_feat] = value\n",
    "\n",
    "\n",
    "        return counterfactual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c9bc4-0d89-44fd-8b81-b2582a7d62bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29063079-1419-4422-b6d9-a3065d401245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler(n_agents=10_000, n_continuous=2, bias_factor=0, mean=0, std=1/4, random_state=None):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    groups = pd.Series(rng.binomial(1,.5, n_agents), name=\"groups\")\n",
    "    counts = Counter(groups)\n",
    "    continuous_cols = [f\"f{i}\" for i in range(n_continuous)]\n",
    "    \n",
    "    # Generate the input dataset\n",
    "    X_0 = pd.DataFrame(\n",
    "        rng.normal(loc=mean, scale=std, size=(counts[0], n_continuous)),\n",
    "        index=groups[groups == 0].index,\n",
    "        columns=continuous_cols,\n",
    "    )\n",
    "\n",
    "    X_1 = pd.DataFrame(\n",
    "        rng.normal(loc=mean+bias_factor*std, scale=std, size=(counts[1], n_continuous)),\n",
    "        index=groups[groups == 1].index,\n",
    "        columns=continuous_cols,\n",
    "    )\n",
    "\n",
    "    X = pd.concat([X_0, X_1]).sort_index()\n",
    "    return MinMaxScaler().fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b0cf70-7dc1-4aa7-90f1-bacd4ecabb7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def biased_data_generator(n_agents, n_continuous=2, bias_factor=0, mean=0, std=1/4, scaler=None, random_state=None):\n",
    "    \"\"\"\n",
    "    groups feature: \n",
    "    - 0 -> Disadvantaged group\n",
    "    - 1 -> Advantaged group\n",
    "    \n",
    "    ``bias_factor`` varies between [0, 1], 0 is completely unbiased, 1 is fully biased.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    groups = pd.Series(rng.binomial(1,.5, n_agents), name=\"groups\")\n",
    "    counts = Counter(groups)\n",
    "    continuous_cols = [f\"f{i}\" for i in range(n_continuous)]\n",
    "\n",
    "    # Generate the input dataset\n",
    "    X_0 = pd.DataFrame(\n",
    "        rng.normal(loc=mean, scale=std, size=(counts[0], n_continuous)),\n",
    "        index=groups[groups == 0].index,\n",
    "        columns=continuous_cols,\n",
    "    )\n",
    "\n",
    "    X_1 = pd.DataFrame(\n",
    "        rng.normal(loc=mean+bias_factor*std, scale=std, size=(counts[1], n_continuous)),\n",
    "        index=groups[groups == 1].index,\n",
    "        columns=continuous_cols,\n",
    "    )\n",
    "\n",
    "    X = pd.concat([X_0, X_1]).sort_index()\n",
    "    \n",
    "    # TEST: scale continuous features\n",
    "    if scaler is not None:\n",
    "        X.loc[:,:] = scaler.transform(X)\n",
    "    \n",
    "    X = pd.concat([X, groups], axis=1)\n",
    "    X = np.clip(X, 0, 1)\n",
    "    \n",
    "    # Generate the target\n",
    "    p0 = 1 / (2 + 2*bias_factor)\n",
    "    p1 = 1 - p0\n",
    "\n",
    "    y0 = rng.binomial(1, p0, counts[0])\n",
    "    y1 = rng.binomial(1, p1, counts[1])\n",
    "    \n",
    "    y = pd.concat(\n",
    "        [\n",
    "            pd.Series((y0 if val==0 else y1), index=group.index) \n",
    "            for val, group in X.groupby(\"groups\")\n",
    "        ]\n",
    "    ).sort_index()\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c7c00-43aa-4a67-b0e8-c9872fd07ec9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IgnoreGroupLR(LogisticRegression):\n",
    "    def __init__(self, ignore_feature=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ignore_feature = ignore_feature\n",
    "    \n",
    "    def _get_X(self, X):\n",
    "        return X.copy() if self.ignore_feature is None else X.drop(columns=self.ignore_feature)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"NOTE: X must be a pandas dataframe.\"\"\"\n",
    "        super().fit(self._get_X(X), y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return super().predict(self._get_X(X))\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return super().predict_proba(self._get_X(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe208e2-7344-49fe-8337-776c15feb961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from recgame.environments._behavior_functions import ContinuousFlexible\n",
    "\n",
    "\n",
    "class ContinuousConstantCustom(ContinuousFlexible):\n",
    "    \"\"\"Applies continuous adaptation with constant effort.\"\"\"\n",
    "    bias_factor_effort_g0 = 0\n",
    "    bias_factor_effort_g1 = 0\n",
    "    \n",
    "    def effort(self, X, global_adaptation):\n",
    "        \"\"\"\n",
    "        Applies constant effort.\n",
    "\n",
    "        Returns effort rate.\n",
    "        \"\"\"\n",
    "        # Fetch environment variables\n",
    "        rng = self.environment._rng\n",
    "\n",
    "        current_effort = (\n",
    "            self.environment.effort_ if hasattr(self.environment, \"effort_\") else None\n",
    "        )\n",
    "\n",
    "        df_new = (\n",
    "            self.environment._new_agents\n",
    "            if hasattr(self.environment, \"_new_agents\")\n",
    "            else X\n",
    "        )\n",
    "\n",
    "        counts = Counter(df_new[\"groups\"])\n",
    "        \n",
    "        x0 = np.abs(rng.normal(0+self.bias_factor_effort_g0, 1, counts[0]))\n",
    "        x1 = np.abs(rng.normal(0+self.bias_factor_effort_g1, 1, counts[1]))\n",
    "        \n",
    "        x = df_new[\"groups\"].copy()\n",
    "        x.loc[x==0] = x0\n",
    "        x.loc[x==1] = x1\n",
    "        x = x.values\n",
    "\n",
    "        effort_rate = x * global_adaptation / 20\n",
    "        effort_rate = pd.Series(effort_rate, index=df_new.index)\n",
    "        effort_rate = pd.concat([current_effort, effort_rate])\n",
    "\n",
    "        # return pd.Series(effort_rate, index=X.index)\n",
    "        return effort_rate\n",
    "\n",
    "    \n",
    "def behavior_function_generator(bias_factor_effort_g0, bias_factor_effort_g1):\n",
    "    behav = ContinuousConstantCustom\n",
    "    behav.bias_factor_effort_g0 = bias_factor_effort_g0\n",
    "    behav.bias_factor_effort_g1 = bias_factor_effort_g1\n",
    "    return behav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a3ec5-f8d3-46e2-9ce1-55a790648ab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fairness_metrics(environment, bins=10, advantaged_pop=1):\n",
    "    # Get groups\n",
    "    groups = pd.concat([environment.metadata_[step][\"X\"][\"groups\"] for step in environment.metadata_.keys()])\n",
    "    groups = groups[~groups.index.duplicated(keep='last')].sort_index()\n",
    "\n",
    "    # Get time for recourse\n",
    "    agents_info = environment.analysis.agents_info()\n",
    "    agents_info = pd.concat([agents_info, groups], axis=1)\n",
    "    agents_info[\"time_for_recourse\"] = agents_info[\"favorable_step\"] - agents_info[\"entered_step\"]\n",
    "\n",
    "    # Get fairness analysis\n",
    "    fairness_analysis = agents_info.dropna().groupby(\"groups\").mean()\n",
    "    success_rates = environment.analysis.success_rate(filter_feature=\"groups\")\n",
    "    fairness_analysis[\"avg_recourse_reliability\"] = success_rates.mean()\n",
    "\n",
    "    # Get disparity metrics\n",
    "    not_advantaged = fairness_analysis.index[fairness_analysis.index != advantaged_pop]\n",
    "    \n",
    "    if len(not_advantaged) > 1:\n",
    "        raise NotImplementedError(\"Only two groups supported.\")\n",
    "    else:\n",
    "        not_advantaged = not_advantaged[0]\n",
    "        \n",
    "    disparates = (fairness_analysis.loc[not_advantaged] / fairness_analysis.loc[advantaged_pop])\n",
    "    disparates = disparates[[\"time_for_recourse\"]]\n",
    "    disparates.index = [var for var in disparates.index + \"_disparity\"]\n",
    "    disparates[\"time_for_recourse_disparity\"] = (\n",
    "        fairness_analysis.loc[not_advantaged, \"time_for_recourse\"] \n",
    "        - fairness_analysis.loc[advantaged_pop, \"time_for_recourse\"]\n",
    "    )\n",
    "\n",
    "    # Get Equality of Opportunity\n",
    "    init_scores = environment.metadata_[0][\"score\"]\n",
    "    efforts = []\n",
    "    for step in environment.metadata_.keys():\n",
    "        if step == 0:\n",
    "            continue\n",
    "            \n",
    "        eff = (\n",
    "            environment.outcome(step=step, return_scores=True)[-1]\n",
    "            - environment.outcome(step=step-1, return_scores=True)[-1]\n",
    "        )\n",
    "        eff = eff[eff.index.isin(environment.metadata_[step][\"X\"].index)]\n",
    "        eff.fillna(0, inplace=True)\n",
    "        \n",
    "        efforts.append((step, eff))\n",
    "\n",
    "    \n",
    "    # Used to get the features to calculate EO\n",
    "    def extract_info(df):\n",
    "        avg_effort = df[\"effort\"].mean()    \n",
    "        outcome_rate = df[\"outcome\"].sum() / df.shape[0]\n",
    "        return pd.Series({\"avg_effort\": avg_effort, \"outcome_rate\": outcome_rate})\n",
    "\n",
    "    eo_per_step = []\n",
    "    for step, effort in efforts:\n",
    "        ai_step = agents_info.copy()\n",
    "        ai_step[\"effort\"] = effort\n",
    "        ai_step.dropna(subset=\"effort\", inplace=True)    \n",
    "        ai_step[\"effort_bins\"] = pd.cut(ai_step[\"effort\"], bins)\n",
    "        ai_step[\"outcome\"] = environment.metadata_[step][\"outcome\"]\n",
    "        \n",
    "        eo = ai_step.groupby([\"groups\", \"effort_bins\"], group_keys=True).apply(extract_info)\n",
    "        eo = eo.reset_index().groupby(\"groups\", group_keys=True).apply(lambda df: (df[\"outcome_rate\"] * df[\"avg_effort\"]).sum() / df[\"avg_effort\"].sum())\n",
    "        eo = eo.to_frame(step).T\n",
    "        eo[\"eo_total\"] = eo[not_advantaged] / eo[advantaged_pop]\n",
    "        eo_per_step.append(eo)\n",
    "    \n",
    "    eo_per_step = pd.concat(eo_per_step)\n",
    "    \n",
    "    disparates[\"avg_EO\"] = eo_per_step[\"eo_total\"].mean()\n",
    "    \n",
    "    if return_eo_only:\n",
    "        return eo_per_step\n",
    "    \n",
    "    return disparates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a35d36-860c-4b26-928d-8ef022d3114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_metrics_per_time_step(environment):\n",
    "    # Get groups\n",
    "    groups = pd.concat([environment.metadata_[step][\"X\"][\"groups\"] for step in environment.metadata_.keys()])\n",
    "    groups = groups[~groups.index.duplicated(keep='last')].sort_index()\n",
    "\n",
    "    # Get time for recourse\n",
    "    agents_info = environment.analysis.agents_info()\n",
    "    agents_info = pd.concat([agents_info, groups], axis=1)\n",
    "    agents_info[\"time_for_recourse\"] = agents_info[\"favorable_step\"] - agents_info[\"entered_step\"]\n",
    "\n",
    "    # Get fairness analysis\n",
    "    fairness_analysis = agents_info.dropna().groupby(\"groups\").mean()\n",
    "    success_rates = environment.analysis.success_rate(filter_feature=\"groups\")\n",
    "    sns.lineplot(success_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea10f7c-3d04-4d51-ad2e-758fc7cd4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_metrics_overall_visualizations(environment):\n",
    "    # Get groups\n",
    "    groups = pd.concat([environment.metadata_[step][\"X\"][\"groups\"] for step in environment.metadata_.keys()])\n",
    "    groups = groups[~groups.index.duplicated(keep='last')].sort_index()\n",
    "\n",
    "    # Get time for recourse\n",
    "    agents_info = environment.analysis.agents_info()\n",
    "    agents_info = pd.concat([agents_info, groups], axis=1)\n",
    "    agents_info[\"time_for_recourse\"] = agents_info[\"favorable_step\"] - agents_info[\"entered_step\"]\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    # ETR - Effort to recourse\n",
    "    ai_etr = agents_info.dropna(subset=\"favorable_step\")\n",
    "    ai_etr = ai_etr[ai_etr[\"n_adaptations\"]!=0]\n",
    "\n",
    "    ai_etr[\"total_effort\"] = ai_etr[\"final_score\"] - ai_etr[\"original_score\"]\n",
    "    etr = ai_etr.groupby(\"groups\").mean()[\"total_effort\"]\n",
    "    results[\"etr_disparity\"] = etr.loc[0] / etr.loc[1]    \n",
    "    \n",
    "    # TTR\n",
    "    ttr = ai_etr.groupby(\"groups\").mean()[\"time_for_recourse\"]\n",
    "    results[\"disparate_ttr\"] = ttr.loc[0] - ttr.loc[1]\n",
    "\n",
    "    sns.boxplot(data=ai_etr, x=\"groups\", y=\"total_effort\")\n",
    "    plt.show()\n",
    "    \n",
    "    sns.boxplot(data=ai_etr, x=\"groups\", y=\"time_for_recourse\")\n",
    "    plt.show()\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a361376c-65ff-4e9e-945c-ed000405241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_metrics_overall(environment):\n",
    "    # Get groups\n",
    "    groups = pd.concat([environment.metadata_[step][\"X\"][\"groups\"] for step in environment.metadata_.keys()])\n",
    "    groups = groups[~groups.index.duplicated(keep='last')].sort_index()\n",
    "\n",
    "    # Get time for recourse\n",
    "    agents_info = environment.analysis.agents_info()\n",
    "    agents_info = pd.concat([agents_info, groups], axis=1)\n",
    "    agents_info[\"time_for_recourse\"] = agents_info[\"favorable_step\"] - agents_info[\"entered_step\"]\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    # ETR - Effort to recourse\n",
    "    ai_etr = agents_info.dropna(subset=\"favorable_step\")\n",
    "    ai_etr = ai_etr[ai_etr[\"n_adaptations\"]!=0]\n",
    "\n",
    "    ai_etr[\"total_effort\"] = ai_etr[\"final_score\"] - ai_etr[\"original_score\"]\n",
    "    etr = ai_etr.groupby(\"groups\").mean()[\"total_effort\"]\n",
    "    results[\"etr_disparity\"] = etr.loc[0] / etr.loc[1]    \n",
    "    \n",
    "    # TTR\n",
    "    ttr = ai_etr.groupby(\"groups\").mean()[\"time_for_recourse\"]\n",
    "    results[\"disparate_ttr\"] = ttr.loc[0] - ttr.loc[1]\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e77e92-7c47-4319-8dfc-d2c4d1c7c878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fairness_metrics_viz_data(environment):\n",
    "    # Get groups\n",
    "    groups = pd.concat([environment.metadata_[step][\"X\"][\"groups\"] for step in environment.metadata_.keys()])\n",
    "    groups = groups[~groups.index.duplicated(keep='last')].sort_index()\n",
    "\n",
    "    # Get time for recourse\n",
    "    agents_info = environment.analysis.agents_info()\n",
    "    agents_info = pd.concat([agents_info, groups], axis=1)\n",
    "    agents_info[\"time_for_recourse\"] = agents_info[\"favorable_step\"] - agents_info[\"entered_step\"]\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    # ETR - Effort to recourse\n",
    "    ai_etr = agents_info.dropna(subset=\"favorable_step\")\n",
    "    ai_etr = ai_etr[ai_etr[\"n_adaptations\"]!=0]\n",
    "\n",
    "    ai_etr[\"total_effort\"] = ai_etr[\"final_score\"] - ai_etr[\"original_score\"]\n",
    "    etr = ai_etr.groupby(\"groups\").mean()[\"total_effort\"]\n",
    "    results[\"etr_disparity\"] = etr.loc[0] / etr.loc[1]    \n",
    "    \n",
    "    # TTR\n",
    "    ttr = ai_etr.groupby(\"groups\").mean()[\"time_for_recourse\"]\n",
    "    results[\"disparate_ttr\"] = ttr.loc[0] - ttr.loc[1]\n",
    "\n",
    "    return ai_etr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efbdbb7-724b-4699-a927-1e73721a04d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _meta_simulation(params):\n",
    "    \"\"\"\n",
    "    N_AGENTS\n",
    "    N_CONTINUOUS\n",
    "    N_LOANS\n",
    "    BIAS_FACTOR\n",
    "    ADAPTATION\n",
    "    NEW_AGENTS\n",
    "    RNG_SEED\n",
    "    \"\"\" \n",
    "    N_AGENTS, N_CONTINUOUS, N_LOANS, BIAS_FACTOR, ADAPTATION, NEW_AGENTS, BIAS_EFFORT, RNG_SEED = params\n",
    "    \n",
    "    rng = np.random.default_rng(RNG_SEED)\n",
    "    \n",
    "    scaler = get_scaler(\n",
    "        n_agents=10_000, \n",
    "        n_continuous=N_CONTINUOUS, \n",
    "        bias_factor=BIAS_FACTOR, \n",
    "        random_state=rng\n",
    "    )\n",
    "\n",
    "    def env_biased_data_generator(n_agents):\n",
    "        return biased_data_generator(n_agents, n_continuous=N_CONTINUOUS, bias_factor=BIAS_FACTOR, scaler=scaler, random_state=rng)[0]\n",
    "    \n",
    "    df, y = biased_data_generator(N_AGENTS, n_continuous=N_CONTINUOUS, bias_factor=BIAS_FACTOR, scaler=scaler, random_state=rng)\n",
    "    categorical = [\"groups\"]\n",
    "    \n",
    "    model = IgnoreGroupLR(categorical, random_state=RNG_SEED).fit(df, y)\n",
    "    \n",
    "    # Define the necessary components to run simulation\n",
    "    recourse = NFeatureRecourse(model, categorical=[\"groups\"], immutable=[\"groups\"])  # , random_state=RNG_SEED)\n",
    "    recourse.set_actions(df)\n",
    "    recourse.action_set_.lb = [-0.1, -0.1, 0]\n",
    "    recourse.action_set_.ub = [1.1, 1.1, 1]\n",
    "    \n",
    "    environment = BaseEnvironment(\n",
    "        X=df,\n",
    "        recourse=recourse,\n",
    "        data_source_func=env_biased_data_generator,\n",
    "        threshold=N_LOANS,\n",
    "        threshold_type=\"absolute\",\n",
    "        adaptation=ADAPTATION,\n",
    "        behavior_function=behavior_function_generator(*BIAS_EFFORT),\n",
    "        growth_rate=NEW_AGENTS,\n",
    "        growth_rate_type=\"absolute\",\n",
    "        random_state=RNG_SEED,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        environment.simulate(20)\n",
    "        return (\n",
    "            {\n",
    "                \"N_AGENTS\": N_AGENTS,\n",
    "                \"N_CONTINUOUS\": N_CONTINUOUS,\n",
    "                \"N_LOANS\": N_LOANS,\n",
    "                \"BIAS_FACTOR\": BIAS_FACTOR,\n",
    "                \"ADAPTATION\": ADAPTATION,\n",
    "                \"NEW_AGENTS\": NEW_AGENTS,\n",
    "                \"BIAS_EFFORT\": BIAS_EFFORT,\n",
    "                \"RNG_SEED\": RNG_SEED,\n",
    "    \n",
    "            }, \n",
    "            environment,\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Experiment failed with params {params}\\nRetrying with a new random seed.\")\n",
    "        params[\"RNG_SEED\"] = params[\"RNG_SEED\"]+1\n",
    "        return _meta_simulation(params)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596270bc-1ab5-416a-a583-65172cf58829",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b24602-46bb-4e9e-a80a-374c08880de8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_AGENTS = [100]\n",
    "N_CONTINUOUS = [2]\n",
    "N_LOANS = [10]\n",
    "BIAS_FACTOR = [0,1,2,3]\n",
    "ADAPTATION = [.5]\n",
    "NEW_AGENTS = [10]\n",
    "RNG_SEED = check_random_states(42, 5)\n",
    "BIAS_EFFORT = ([1, 0],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76111b56-32ac-4c97-8403-a4c089aacf90",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EXP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a0c657-169d-44c2-bc66-d37af5a3c3e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = parallel_loop(\n",
    "    _meta_simulation, \n",
    "    list(product(\n",
    "        N_AGENTS,\n",
    "        N_CONTINUOUS,\n",
    "        N_LOANS,\n",
    "        BIAS_FACTOR,\n",
    "        ADAPTATION,\n",
    "        NEW_AGENTS,\n",
    "        BIAS_EFFORT,\n",
    "        RNG_SEED,\n",
    "    )),\n",
    "    n_jobs=-1,\n",
    "    progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb63996-2a8a-4a16-9193-af05d869e21a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fair_res = []\n",
    "# for hyperparams, environment in results:\n",
    "#     bias = hyperparams[\"BIAS_FACTOR\"]\n",
    "#     fairness_res = fairness_metrics(environment, filter_feature=\"groups\", bins=10, advantaged_pop=1)\n",
    "#     fairness_res[\"Merit\"] = bias\n",
    "#     fair_res.append(fairness_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c953d-8799-4b5a-9217-64309c66dcbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(np.mean(ai_etrs, axis=0), columns=ai_etrs[0].columns, index=ai_etrs[0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a021c-fa8d-4df5-bbcc-a918ed9afec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics_dfs = []\n",
    "for params, environment in results:\n",
    "    ai_etr = fairness_metrics_viz_data(environment)\n",
    "    ai_etr[\"rng_seed\"] = params[\"RNG_SEED\"]\n",
    "    metrics_dfs.append((params, ai_etr))\n",
    "    \n",
    "ai_etrs_all = []\n",
    "for bf in BIAS_FACTOR:\n",
    "    ai_etrs = [df for p, df in metrics_dfs if p[\"BIAS_FACTOR\"] == bf]\n",
    "    ai_etrs = pd.concat(ai_etrs)\n",
    "    ai_etrs[\"qualification\"] = bf\n",
    "    ai_etrs_all.append(ai_etrs)\n",
    "\n",
    "agents_data_n_runs = pd.concat(ai_etrs_all)\n",
    "\n",
    "agents_data_n_runs[\"groups2\"] = agents_data_n_runs[\"groups\"].apply(lambda x: \"Adv.\" if x == 1 else \"Disadv.\")\n",
    "\n",
    "ax = sns.boxplot(data=agents_data_n_runs, x=\"qualification\", y=\"total_effort\", hue=\"groups2\")\n",
    "ax.set_xlabel(\"Qualification\")\n",
    "ax.set_ylabel(\"Total effort\")\n",
    "sns.move_legend(\n",
    "    ax, \"lower center\", bbox_to_anchor=(.5, 1), ncol=3, title=None, frameon=False\n",
    ")\n",
    "# plt.savefig(\n",
    "#     f\"B10_total_effort.pdf\",\n",
    "#     format=\"pdf\",\n",
    "#     bbox_inches=\"tight\",\n",
    "# )\n",
    "plt.show()\n",
    "\n",
    "ax = sns.boxplot(data=agents_data_n_runs, x=\"qualification\", y=\"time_for_recourse\", hue=\"groups2\")\n",
    "ax.set_xlabel(\"Qualification\")\n",
    "ax.set_ylabel(\"Total time\")\n",
    "sns.move_legend(\n",
    "    ax, \"lower center\", bbox_to_anchor=(.5, 1), ncol=3, title=None, frameon=False,\n",
    ")\n",
    "# plt.savefig(\n",
    "#     f\"B10_time_for_recourse.pdf\",\n",
    "#     format=\"pdf\",\n",
    "#     bbox_inches=\"tight\",\n",
    "# )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fa9dc0-7df6-4a93-b937-b5a3374fa316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agents_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb2d455-db2d-401c-8bc0-71ba4dde8f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1156c1f-6653-4d61-a5b7-2639d3e766ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "QUALIFICATION = 1\n",
    "\n",
    "adnr = agents_data_n_runs[agents_data_n_runs.qualification == QUALIFICATION]\n",
    "\n",
    "data = (\n",
    "    adnr\n",
    "    .groupby([\"rng_seed\", \"favorable_step\", \"groups2\"])[\"total_effort\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .pivot(index=[\"rng_seed\", \"favorable_step\"] , columns=\"groups2\", values=\"total_effort\")\n",
    "    .reset_index()\n",
    "    .drop(columns=\"rng_seed\")\n",
    "    .set_index(\"favorable_step\")\n",
    ")\n",
    "\n",
    "ax = sns.lineplot(data)\n",
    "ax.set_xlabel(\"Time-step\")\n",
    "ax.set_ylabel(\"ETR\")\n",
    "sns.move_legend(\n",
    "    ax, \"lower center\", bbox_to_anchor=(.5, 1), ncol=3, title=None, frameon=False,\n",
    ")\n",
    "# plt.savefig(\n",
    "#     f\"Q1_ETR_over_time.pdf\",\n",
    "#     format=\"pdf\",\n",
    "#     bbox_inches=\"tight\",\n",
    "# )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e733c54-e35d-4ef0-8186-64508ac4d6d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m1 = agents_data_n_runs.groupby([\"qualification\", \"groups\"]).mean(numeric_only=True)[[\"time_for_recourse\", \"total_effort\"]]\n",
    "\n",
    "def apply_transf(df):\n",
    "    df = df.set_index(\"groups\")\n",
    "    res_ = {}\n",
    "    res_[\"time_to_recourse_difference\"] = (df.loc[0, \"time_for_recourse\"] - df.loc[1, \"time_for_recourse\"])\n",
    "    res_[\"effort_to_recourse_ratio\"] = (df.loc[0, \"total_effort\"] / df.loc[1, \"total_effort\"])\n",
    "    return pd.Series(res_)\n",
    "\n",
    "m1.reset_index().groupby(\"qualification\").apply(apply_transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139ae37e-d4d6-4aea-addd-1c8a891dc89e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EXP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4c751-1bac-46bc-bd2a-7483ab3aaefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIAS_EFFORT2 = ([0, 1],) # [0, 0], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d74a7-13de-4cf0-9ac0-8dc8896182a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = parallel_loop(\n",
    "    _meta_simulation, \n",
    "    list(product(\n",
    "        N_AGENTS,\n",
    "        N_CONTINUOUS,\n",
    "        N_LOANS,\n",
    "        BIAS_FACTOR,\n",
    "        ADAPTATION,\n",
    "        NEW_AGENTS,\n",
    "        BIAS_EFFORT2,\n",
    "        RNG_SEED,\n",
    "    )),\n",
    "    n_jobs=-1,\n",
    "    progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45df665-15a4-4691-aa92-74f8999573ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fair_res = []\n",
    "# for hyperparams, environment in results2:\n",
    "#     bias = hyperparams[\"BIAS_FACTOR\"]\n",
    "#     fairness_res = fairness_metrics(environment, filter_feature=\"groups\", bins=10, advantaged_pop=1)\n",
    "#     fairness_res[\"Merit\"] = bias\n",
    "#     fair_res.append(fairness_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7838b265-d750-45f1-8d9b-dcc6fe07af46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics_dfs = []\n",
    "for params, environment in results2:\n",
    "    ai_etr = fairness_metrics_viz_data(environment)\n",
    "    metrics_dfs.append((params, ai_etr))\n",
    "    \n",
    "ai_etrs_all = []\n",
    "for bf in BIAS_FACTOR:\n",
    "    ai_etrs = [df for p, df in metrics_dfs if p[\"BIAS_FACTOR\"] == bf]\n",
    "    ai_etrs = pd.concat(ai_etrs)\n",
    "    ai_etrs[\"qualification\"] = bf\n",
    "    ai_etrs_all.append(ai_etrs)\n",
    "\n",
    "agents_data_n_runs = pd.concat(ai_etrs_all)\n",
    "\n",
    "agents_data_n_runs[\"groups2\"] = agents_data_n_runs[\"groups\"].apply(lambda x: \"Adv.\" if x == 1 else \"Disadv.\")\n",
    "\n",
    "ax = sns.boxplot(data=agents_data_n_runs, x=\"qualification\", y=\"total_effort\", hue=\"groups2\")        \n",
    "ax.set_xlabel(\"Qualification\")\n",
    "ax.set_ylabel(\"Total effort\")\n",
    "sns.move_legend(\n",
    "    ax, \"lower center\", bbox_to_anchor=(.5, 1), ncol=3, title=None, frameon=False\n",
    ")\n",
    "# plt.savefig(\n",
    "#     f\"B01_total_effort.pdf\",\n",
    "#     format=\"pdf\",\n",
    "#     bbox_inches=\"tight\",\n",
    "# )\n",
    "plt.show()\n",
    "\n",
    "ax = sns.boxplot(data=agents_data_n_runs, x=\"qualification\", y=\"time_for_recourse\", hue=\"groups2\")\n",
    "ax.set_xlabel(\"Qualification\")\n",
    "ax.set_ylabel(\"Total time\")\n",
    "sns.move_legend(\n",
    "    ax, \"lower center\", bbox_to_anchor=(.5, 1), ncol=3, title=None, frameon=False,\n",
    ")\n",
    "# plt.savefig(\n",
    "#     f\"B01_time_for_recourse.pdf\",\n",
    "#     format=\"pdf\",\n",
    "#     bbox_inches=\"tight\",\n",
    "# )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346d7507-7ed2-40a3-a07b-eedf19c201b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m2 = agents_data_n_runs.groupby([\"qualification\", \"groups\"]).mean(numeric_only=True)[[\"time_for_recourse\", \"total_effort\"]]\n",
    "\n",
    "def apply_transf(df):\n",
    "    df = df.set_index(\"groups\")\n",
    "    res_ = {}\n",
    "    res_[\"time_to_recourse_difference\"] = (df.loc[0, \"time_for_recourse\"] - df.loc[1, \"time_for_recourse\"])\n",
    "    res_[\"effort_to_recourse_ratio\"] = (df.loc[0, \"total_effort\"] / df.loc[1, \"total_effort\"])\n",
    "    return pd.Series(res_)\n",
    "\n",
    "m2.reset_index().groupby(\"qualification\").apply(apply_transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3879134-3705-4d6f-b90d-e0757d5b7007",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EXP 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f67be-f102-4d63-8b52-bd7d463e9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIAS_EFFORT3 = ([0, 0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad8dec-1fec-4b53-93d5-3c9d1b9bf248",
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = parallel_loop(\n",
    "    _meta_simulation, \n",
    "    list(product(\n",
    "        N_AGENTS,\n",
    "        N_CONTINUOUS,\n",
    "        N_LOANS,\n",
    "        BIAS_FACTOR,\n",
    "        ADAPTATION,\n",
    "        NEW_AGENTS,\n",
    "        BIAS_EFFORT3,\n",
    "        RNG_SEED,\n",
    "    )),\n",
    "    n_jobs=-1,\n",
    "    progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72035ca-ca2e-4958-99e3-6b8de8989b74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fair_res = []\n",
    "# for hyperparams, environment in results:\n",
    "#     bias = hyperparams[\"BIAS_FACTOR\"]\n",
    "#     fairness_res = fairness_metrics(environment, filter_feature=\"groups\", bins=10, advantaged_pop=1)\n",
    "#     fairness_res[\"Merit\"] = bias\n",
    "#     fair_res.append(fairness_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5fe7d8-bfe3-4bbb-a068-d8d57fcb49d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics_dfs = []\n",
    "for params, environment in results3:\n",
    "    ai_etr = fairness_metrics_viz_data(environment)\n",
    "    metrics_dfs.append((params, ai_etr))\n",
    "    \n",
    "ai_etrs_all = []\n",
    "for bf in BIAS_FACTOR:\n",
    "    ai_etrs = [df for p, df in metrics_dfs if p[\"BIAS_FACTOR\"] == bf]\n",
    "    ai_etrs = pd.concat(ai_etrs)\n",
    "    ai_etrs[\"qualification\"] = bf\n",
    "    ai_etrs_all.append(ai_etrs)\n",
    "\n",
    "agents_data_n_runs = pd.concat(ai_etrs_all)\n",
    "\n",
    "agents_data_n_runs[\"groups2\"] = agents_data_n_runs[\"groups\"].apply(lambda x: \"Adv.\" if x == 1 else \"Disadv.\")\n",
    "\n",
    "ax = sns.boxplot(data=agents_data_n_runs, x=\"qualification\", y=\"total_effort\", hue=\"groups2\")\n",
    "ax.set_xlabel(\"Qualification\")\n",
    "ax.set_ylabel(\"Total effort\")\n",
    "sns.move_legend(\n",
    "    ax, \"lower center\", bbox_to_anchor=(.5, 1), ncol=3, title=None, frameon=False\n",
    ")\n",
    "# plt.savefig(\n",
    "#     f\"B00_total_effort.pdf\",\n",
    "#     format=\"pdf\",\n",
    "#     bbox_inches=\"tight\",\n",
    "# )\n",
    "plt.show()\n",
    "\n",
    "ax = sns.boxplot(data=agents_data_n_runs, x=\"qualification\", y=\"time_for_recourse\", hue=\"groups2\")\n",
    "ax.set_xlabel(\"Qualification\")\n",
    "ax.set_ylabel(\"Total time\")\n",
    "sns.move_legend(\n",
    "    ax, \"lower center\", bbox_to_anchor=(.5, 1), ncol=3, title=None, frameon=False,\n",
    ")\n",
    "# plt.savefig(\n",
    "#     f\"B00_time_for_recourse.pdf\",\n",
    "#     format=\"pdf\",\n",
    "#     bbox_inches=\"tight\",\n",
    "# )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cce152-1992-4da3-a8d8-1f118c0ce63d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m3 = agents_data_n_runs.groupby([\"qualification\", \"groups\"]).mean(numeric_only=True)[[\"time_for_recourse\", \"total_effort\"]]\n",
    "\n",
    "def apply_transf(df):\n",
    "    df = df.set_index(\"groups\")\n",
    "    res_ = {}\n",
    "    res_[\"time_to_recourse_difference\"] = (df.loc[0, \"time_for_recourse\"] - df.loc[1, \"time_for_recourse\"])\n",
    "    res_[\"effort_to_recourse_ratio\"] = (df.loc[0, \"total_effort\"] / df.loc[1, \"total_effort\"])\n",
    "    return pd.Series(res_)\n",
    "\n",
    "m3.reset_index().groupby(\"qualification\").apply(apply_transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c74c12-11e0-4632-9e8d-ca92cdb63d63",
   "metadata": {},
   "source": [
    "# 1st PAGE VIZ - TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5786d153-d7d4-419d-bc30-427f0d1ce6df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "QUALIFICATION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf19f5c-47ea-459c-b65c-31e5e8053299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores_dfs = []\n",
    "for params, environment in results:\n",
    "    if params[\"BIAS_FACTOR\"] == QUALIFICATION:\n",
    "        scores = [ \n",
    "            pd.concat(\n",
    "                [\n",
    "                    environment.metadata_[step][\"score\"].rename(\"score\"), \n",
    "                    environment.metadata_[step][\"X\"][\"groups\"],\n",
    "                    pd.Series(np.ones(environment.metadata_[step][\"score\"].shape), name=\"step\") * step\n",
    "                ], \n",
    "                axis=1\n",
    "            ).groupby(\"groups\").mean()\n",
    "            for step in environment.metadata_.keys()\n",
    "        ]\n",
    "        \n",
    "        scores = pd.concat(scores).reset_index()\n",
    "        scores[\"groups\"] = scores[\"groups\"].astype(int)\n",
    "        scores[\"step\"] = scores[\"step\"].astype(int)\n",
    "        scores = scores.pivot(index=\"step\", columns=\"groups\", values=\"score\").reset_index()\n",
    "        scores.columns.rename(None, inplace=True)\n",
    "        scores_dfs.append(scores)\n",
    "        \n",
    "scores_dfs_exp1 = pd.DataFrame(np.mean(scores_dfs, axis=0), columns=scores_dfs[0].columns, index=scores_dfs[0].index)\n",
    "scores_dfs_exp1[\"step\"] = scores_dfs_exp1[\"step\"].astype(int)\n",
    "scores_dfs_exp1 = scores_dfs_exp1.set_index(\"step\")\n",
    "ax = sns.lineplot(scores_dfs_exp1)\n",
    "ax.set_xlabel(\"Time-step\")\n",
    "ax.set_ylabel(\"Mean score\")\n",
    "sns.move_legend(\n",
    "    ax, \"lower center\", bbox_to_anchor=(.5, 1), ncol=3, title=None, frameon=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d811a376-9b32-423a-9127-62b5459cdb48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "QUALIFICATION = 2\n",
    "\n",
    "adnr = agents_data_n_runs[agents_data_n_runs.qualification == QUALIFICATION]\n",
    "\n",
    "steps_data = []\n",
    "for step in range(int(adnr.favorable_step.max()+1)):\n",
    "    agents_outcome = adnr[adnr[\"favorable_step\"].astype(int) == step]\n",
    "    etr = agents_outcome.groupby(\"groups2\").size()\n",
    "    etr.name = step\n",
    "    steps_data.append(etr)\n",
    "    \n",
    "steps_data = pd.concat(steps_data, axis=1).T\n",
    "steps_data.columns.rename(None, inplace=True)\n",
    "ax = sns.lineplot(steps_data)\n",
    "ax.set_xlabel(\"Time-step\")\n",
    "ax.set_ylabel(\"\")\n",
    "sns.move_legend(\n",
    "    ax, \"lower center\", bbox_to_anchor=(.5, 1), ncol=3, title=None, frameon=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
